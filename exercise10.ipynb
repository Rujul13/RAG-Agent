{"cells":[{"source":["# Putting together a multi-agent system\n","\n","Time to combine everything we've learned into our own multi-agent system!\n","\n","Our Deep Research multi-agent system will have three agents:\n","* ‚ùì `QuestionAgent` which accepts a research topic and generates a bunch of questions\n","* üí¨ `AnswerAgent` which answers a specific question (we'll need to call it many times)\n","* üìù `ReportAgent` which aggregates all the answers and generates a report.\n","\n","We'll create these as `FunctionAgent`s, the same as we did when creating agents for `AgentWorkflow`."],"metadata":{"id":"7c8b40ad-e143-4b27-abc5-e9c42a3bb09f"},"id":"7c8b40ad-e143-4b27-abc5-e9c42a3bb09f","cell_type":"markdown"},{"source":["Running Pre Requisites"],"metadata":{"id":"c6d6729b-eb5e-4912-8b70-82171821bcfe"},"id":"c6d6729b-eb5e-4912-8b70-82171821bcfe","cell_type":"markdown"},{"source":["!pip install llama-index -q -q"],"metadata":{"executionCancelledAt":null,"executionTime":3214,"lastExecutedAt":1756669771155,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install llama-index -q -q","jupyter":{"outputs_hidden":false,"source_hidden":false},"outputsMetadata":{"0":{"height":80,"type":"stream"}},"id":"81488b87-4313-4783-9c84-9e25487a7ab1"},"id":"81488b87-4313-4783-9c84-9e25487a7ab1","cell_type":"code","execution_count":null,"outputs":[]},{"source":["!pip install tavily-python -q -q"],"metadata":{"executionCancelledAt":null,"executionTime":3008,"lastExecutedAt":1756669774165,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install tavily-python -q -q","jupyter":{"outputs_hidden":false,"source_hidden":false},"id":"be69d223-cc23-4fca-ac67-247c11ec5658"},"id":"be69d223-cc23-4fca-ac67-247c11ec5658","cell_type":"code","execution_count":null,"outputs":[]},{"source":["from tavily import AsyncTavilyClient\n","from llama_index.core.agent.workflow import AgentWorkflow\n","from llama_index.llms.openai import OpenAI\n","import os\n","from openai import OpenAI as OpenAIClient\n","\n","raw_client = OpenAIClient()\n","\n","API_KEY   = raw_client.api_key\n","API_BASE  = raw_client.base_url\n","\n","tavily_api_key = os.environ[\"TAVILY_API_KEY\"]\n","\n","async def search_web(query: str) -> str:\n","    \"\"\"Useful for using the web to answer questions.\"\"\"\n","    client = AsyncTavilyClient(api_key=tavily_api_key)\n","    return str(await client.search(query))\n","\n","llm = OpenAI(model=\"gpt-4o-mini\", api_key=API_KEY, api_base=API_BASE)"],"metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1756669774223,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from tavily import AsyncTavilyClient\nfrom llama_index.core.agent.workflow import AgentWorkflow\nfrom llama_index.llms.openai import OpenAI\nimport os\nfrom openai import OpenAI as OpenAIClient\n\nraw_client = OpenAIClient()\n\nAPI_KEY   = raw_client.api_key   \nAPI_BASE  = raw_client.base_url\n\ntavily_api_key = os.environ[\"TAVILY_API_KEY\"]\n\nasync def search_web(query: str) -> str:\n    \"\"\"Useful for using the web to answer questions.\"\"\"\n    client = AsyncTavilyClient(api_key=tavily_api_key)\n    return str(await client.search(query))\n\nllm = OpenAI(model=\"gpt-4o-mini\", api_key=API_KEY, api_base=API_BASE)","jupyter":{"outputs_hidden":false,"source_hidden":false},"id":"b2dbc8c5-affd-4ca7-8966-1e5246ce43b6"},"id":"b2dbc8c5-affd-4ca7-8966-1e5246ce43b6","cell_type":"code","execution_count":null,"outputs":[]},{"source":["from llama_index.core.agent.workflow import FunctionAgent\n","from llama_index.llms.openai import OpenAI\n","import os\n","\n","question_agent = FunctionAgent(\n","    tools=[],\n","    llm=llm,\n","    verbose=False,\n","    system_prompt=\"\"\"You are part of a deep research system.\n","      Given a research topic, you should come up with a bunch of questions\n","      that a separate agent will answer in order to write a comprehensive\n","      report on that topic. To make it easy to answer the questions separately,\n","      you should provide the questions one per line. Don't include markdown\n","      or any preamble in your response, just a list of questions.\"\"\"\n",")\n","answer_agent = FunctionAgent(\n","    tools=[search_web],\n","    llm=llm,\n","    verbose=False,\n","    system_prompt=\"\"\"You are part of a deep research system.\n","      Given a specific question, your job is to come up with a deep answer\n","      to that question, which will be combined with other answers on the topic\n","      into a comprehensive report. You can search the web to get information\n","      on the topic, as many times as you need.\"\"\"\n",")\n","report_agent = FunctionAgent(\n","    tools=[],\n","    llm=llm,\n","    verbose=False,\n","    system_prompt=\"\"\"You are part of a deep research system.\n","      Given a set of answers to a set of questions, your job is to combine\n","      them all into a comprehensive report on the topic.\"\"\"\n",")"],"metadata":{"executionCancelledAt":null,"executionTime":59,"lastExecutedAt":1756669774283,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.llms.openai import OpenAI\nimport os\n\nquestion_agent = FunctionAgent(\n    tools=[],\n    llm=llm,\n    verbose=False,\n    system_prompt=\"\"\"You are part of a deep research system.\n      Given a research topic, you should come up with a bunch of questions\n      that a separate agent will answer in order to write a comprehensive\n      report on that topic. To make it easy to answer the questions separately,\n      you should provide the questions one per line. Don't include markdown\n      or any preamble in your response, just a list of questions.\"\"\"\n)\nanswer_agent = FunctionAgent(\n    tools=[search_web],\n    llm=llm,\n    verbose=False,\n    system_prompt=\"\"\"You are part of a deep research system.\n      Given a specific question, your job is to come up with a deep answer\n      to that question, which will be combined with other answers on the topic\n      into a comprehensive report. You can search the web to get information\n      on the topic, as many times as you need.\"\"\"\n)\nreport_agent = FunctionAgent(\n    tools=[],\n    llm=llm,\n    verbose=False,\n    system_prompt=\"\"\"You are part of a deep research system.\n      Given a set of answers to a set of questions, your job is to combine\n      them all into a comprehensive report on the topic.\"\"\"\n)","id":"e0207f31-f5fc-44ca-a69a-099977835401"},"id":"e0207f31-f5fc-44ca-a69a-099977835401","cell_type":"code","execution_count":null,"outputs":[]},{"source":["The Workflow we'll need to handle this task needs to do a few things:\n","* Accept the topic and pass it to the QuestionAgent\n","* Take all the answers from the QuestionAgent and split them up, firing off one AnswerAgent for each question\n","* Aggregate all the questions and answers from the AnswerAgents\n","* Generate a single report from them"],"metadata":{"id":"89bf31dc-4500-4496-8cf9-362c21ef365e"},"id":"89bf31dc-4500-4496-8cf9-362c21ef365e","cell_type":"markdown"},{"source":["from llama_index.core.workflow import Event, Context\n","from llama_index.core.workflow import (\n","    StartEvent,\n","    StopEvent,\n","    Workflow,\n","    step\n",")\n","\n","class GenerateEvent(Event):\n","    research_topic: str\n","\n","class QuestionEvent(Event):\n","    question: str\n","\n","class AnswerEvent(Event):\n","    question: str\n","    answer: str\n","\n","class ProgressEvent(Event):\n","    msg: str\n","\n","class DeepResearchWorkflow(Workflow):\n","\n","    @step\n","    async def setup(self, ctx: Context, ev: StartEvent) -> GenerateEvent:\n","        self.question_agent = ev.question_agent\n","        self.answer_agent = ev.answer_agent\n","        self.report_agent = ev.report_agent\n","\n","        ctx.write_event_to_stream(ProgressEvent(msg=\"Starting research\"))\n","\n","        return GenerateEvent(research_topic=ev.research_topic)\n","\n","    @step\n","    async def generate_questions(self, ctx: Context, ev: GenerateEvent) -> QuestionEvent:\n","\n","        # CODE: set up context in the deep research workflow\n","        await ctx.set(\"research_topic\", ev.research_topic)\n","        ctx.write_event_to_stream(ProgressEvent(msg=f\"Research topic is {ev.research_topic}\"))\n","\n","        # CODE: set the question agent\n","        result = await self.question_agent.run(user_msg=f\"\"\"Generate some questions\n","          on the topic <topic>{ev.research_topic}</topic>.\"\"\")\n","\n","        # Some basic string manipulation to get separate questions\n","        lines = str(result).split(\"\\n\")\n","        questions = [line.strip() for line in lines if line.strip() != \"\"]\n","\n","        # Record how many answers we're going to need to wait for\n","        await ctx.set(\"total_questions\", len(questions))\n","\n","        # Fire off multiple Answer Agents\n","        for question in questions:\n","            ctx.send_event(QuestionEvent(question=question))\n","\n","    @step\n","    async def answer_question(self, ctx: Context, ev: QuestionEvent) -> AnswerEvent:\n","\n","        # CODE: set the answer agent\n","        result = await self.answer_agent.run(user_msg=f\"\"\"Research the answer to this\n","          question: <question>{ev.question}</question>. You can use web\n","          search to help you find information on the topic, as many times\n","          as you need. Return just the answer without preamble or markdown.\"\"\")\n","\n","        ctx.write_event_to_stream(ProgressEvent(msg=f\"\"\"Received question {ev.question}\n","            Came up with answer: {str(result)}\"\"\"))\n","\n","        return AnswerEvent(question=ev.question,answer=str(result))\n","\n","    @step\n","    async def write_report(self, ctx: Context, ev: AnswerEvent) -> StopEvent:\n","\n","        research = ctx.collect_events(ev, [AnswerEvent] * await ctx.get(\"total_questions\"))\n","        # If we haven't received all the answers yet, this will be None\n","        if research is None:\n","            ctx.write_event_to_stream(ProgressEvent(msg=\"Collecting answers...\"))\n","            return None\n","\n","        ctx.write_event_to_stream(ProgressEvent(msg=\"Generating report...\"))\n","\n","        # Aggregate the questions and answers\n","        all_answers = \"\"\n","        for q_and_a in research:\n","            all_answers += f\"Question: {q_and_a.question}\\nAnswer: {q_and_a.answer}\\n\\n\"\n","\n","        # Prompt the report\n","        result = await self.report_agent.run(user_msg=f\"\"\"You are part of a deep research system.\n","          You have been given a complex topic on which to write a report:\n","          <topic>{await ctx.get(\"research_topic\")}.\n","\n","          Other agents have already come up with a list of questions about the\n","          topic and answers to those questions. Your job is to write a clear,\n","          thorough report that combines all the information from those answers.\n","\n","          Here are the questions and answers:\n","          <questions_and_answers>{all_answers}</questions_and_answers>\"\"\")\n","\n","        return StopEvent(result=str(result))"],"metadata":{"executionCancelledAt":null,"executionTime":63,"lastExecutedAt":1756669774347,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from llama_index.core.workflow import Event, Context\nfrom llama_index.core.workflow import (\n    StartEvent,\n    StopEvent,\n    Workflow,\n    step\n)\n\nclass GenerateEvent(Event):\n    research_topic: str\n\nclass QuestionEvent(Event):\n    question: str\n\nclass AnswerEvent(Event):\n    question: str\n    answer: str\n\nclass ProgressEvent(Event):\n    msg: str\n\nclass DeepResearchWorkflow(Workflow):\n\n    @step\n    async def setup(self, ctx: Context, ev: StartEvent) -> GenerateEvent:\n        self.question_agent = ev.question_agent\n        self.answer_agent = ev.answer_agent\n        self.report_agent = ev.report_agent\n\n        ctx.write_event_to_stream(ProgressEvent(msg=\"Starting research\"))\n\n        return GenerateEvent(research_topic=ev.research_topic)\n\n    @step\n    async def generate_questions(self, ctx: Context, ev: GenerateEvent) -> QuestionEvent:\n\n        # CODE: set up context in the deep research workflow\n        await ctx.set(\"research_topic\", ev.research_topic)\n        ctx.write_event_to_stream(ProgressEvent(msg=f\"Research topic is {ev.research_topic}\"))\n\n        # CODE: set the question agent\n        result = await self.question_agent.run(user_msg=f\"\"\"Generate some questions\n          on the topic <topic>{ev.research_topic}</topic>.\"\"\")\n\n        # Some basic string manipulation to get separate questions\n        lines = str(result).split(\"\\n\")\n        questions = [line.strip() for line in lines if line.strip() != \"\"]\n\n        # Record how many answers we're going to need to wait for\n        await ctx.set(\"total_questions\", len(questions))\n\n        # Fire off multiple Answer Agents\n        for question in questions:\n            ctx.send_event(QuestionEvent(question=question))\n\n    @step\n    async def answer_question(self, ctx: Context, ev: QuestionEvent) -> AnswerEvent:\n\n        # CODE: set the answer agent\n        result = await self.answer_agent.run(user_msg=f\"\"\"Research the answer to this\n          question: <question>{ev.question}</question>. You can use web\n          search to help you find information on the topic, as many times\n          as you need. Return just the answer without preamble or markdown.\"\"\")\n\n        ctx.write_event_to_stream(ProgressEvent(msg=f\"\"\"Received question {ev.question}\n            Came up with answer: {str(result)}\"\"\"))\n\n        return AnswerEvent(question=ev.question,answer=str(result))\n\n    @step\n    async def write_report(self, ctx: Context, ev: AnswerEvent) -> StopEvent:\n\n        research = ctx.collect_events(ev, [AnswerEvent] * await ctx.get(\"total_questions\"))\n        # If we haven't received all the answers yet, this will be None\n        if research is None:\n            ctx.write_event_to_stream(ProgressEvent(msg=\"Collecting answers...\"))\n            return None\n\n        ctx.write_event_to_stream(ProgressEvent(msg=\"Generating report...\"))\n\n        # Aggregate the questions and answers\n        all_answers = \"\"\n        for q_and_a in research:\n            all_answers += f\"Question: {q_and_a.question}\\nAnswer: {q_and_a.answer}\\n\\n\"\n\n        # Prompt the report\n        result = await self.report_agent.run(user_msg=f\"\"\"You are part of a deep research system.\n          You have been given a complex topic on which to write a report:\n          <topic>{await ctx.get(\"research_topic\")}.\n\n          Other agents have already come up with a list of questions about the\n          topic and answers to those questions. Your job is to write a clear,\n          thorough report that combines all the information from those answers.\n\n          Here are the questions and answers:\n          <questions_and_answers>{all_answers}</questions_and_answers>\"\"\")\n\n        return StopEvent(result=str(result))","id":"7e465849-d953-4077-9fd5-074663889b64"},"id":"7e465849-d953-4077-9fd5-074663889b64","cell_type":"code","execution_count":null,"outputs":[]},{"source":["import json\n","import os\n","import asyncio\n","\n","CACHE_PATH = \"sf_history_cache.json\"\n","CACHE_KEY = \"History of San Francisco\"\n","\n","# Load or initialize the cache\n","if os.path.exists(CACHE_PATH):\n","    with open(CACHE_PATH, \"r\") as f:\n","        full_cache = json.load(f)\n","else:\n","    full_cache = {}\n","\n","if CACHE_KEY in full_cache:\n","    cached_result = full_cache"],"metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1756669774399,"lastExecutedByKernel":"33b071b2-d249-4ee3-9701-535a2ab8f56e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import json\nimport os\nimport asyncio\n\nCACHE_PATH = \"sf_history_cache.json\"\nCACHE_KEY = \"History of San Francisco\"\n\n# Load or initialize the cache\nif os.path.exists(CACHE_PATH):\n    with open(CACHE_PATH, \"r\") as f:\n        full_cache = json.load(f)\nelse:\n    full_cache = {}\n\nif CACHE_KEY in full_cache:\n    cached_result = full_cache","jupyter":{"outputs_hidden":false,"source_hidden":false},"id":"c77f20b9-36c6-47d6-8bf3-84fe712574fe"},"id":"c77f20b9-36c6-47d6-8bf3-84fe712574fe","cell_type":"code","execution_count":null,"outputs":[]},{"source":[" cached the report for the topic \"History of San Francisco\" to keep this solution fast so that you can see the outcome instantly.\n","\n","The code below will use the actual agents to generate the report from scratch.\n","\n","```\n","workflow = DeepResearchWorkflow(timeout=300)\n","handler = workflow.run(\n","    research_topic=\"History of San Francisco\",\n","    question_agent=question_agent,\n","    answer_agent=answer_agent,\n","    report_agent=report_agent\n",")\n","\n","async for ev in handler.stream_events():\n","    if isinstance(ev, ProgressEvent):\n","        print(ev.msg)\n","\n","final_result = await handler\n","print(\"==== The report ====\")\n","print(final_result)\n","```"],"metadata":{"id":"69e720ff-cdbd-4af9-b3c7-c236fe0a538c"},"id":"69e720ff-cdbd-4af9-b3c7-c236fe0a538c","cell_type":"markdown"},{"source":["Let's take a look at how running it would get us a sequence of progress events plus our final report:"],"metadata":{"id":"188085b7-4fcd-4f9d-b9b4-cb78ef80b442"},"id":"188085b7-4fcd-4f9d-b9b4-cb78ef80b442","cell_type":"markdown"},{"source":["async def replay_cached():\n","    messages = [\n","        \"Starting research...\",\n","        f\"Research topic is: {CACHE_KEY}\",\n","        \"Collecting answers...\",\n","        \"Generating report...\"\n","    ]\n","    for msg in messages:\n","        await asyncio.sleep(0.5)\n","        print(msg)\n","\n","    print(\"==== The report ====\")\n","    print(cached_result)\n","\n","await replay_cached()"],"metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":616,"type":"stream"}},"id":"afbfe38a-da46-44bf-8aff-2624ec9b8457","outputId":"f8e1e297-91a5-4efc-c588-4dfdf6ad4039"},"id":"afbfe38a-da46-44bf-8aff-2624ec9b8457","cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Starting research...\nResearch topic is: History of San Francisco\nCollecting answers...\nGenerating report...\n==== The report ====\n# Comprehensive Report on the History of San Francisco\n\nSan Francisco, a city known for its iconic landmarks and vibrant culture, has a rich and complex history shaped by various events, movements, and demographic changes. This report explores the key historical milestones, social movements, economic transformations, and cultural developments that have defined San Francisco from its founding to the present day.\n\n## Founding and Early Development\n\nThe history of San Francisco begins long before European contact, with the Ohlone people inhabiting the region for thousands of years. The first significant European interest in the area began with Spanish exploration, notably in 1775 when the Spanish ship San Carlos entered San Francisco Bay. This led to the establishment of the Presidio of San Francisco and Mission San Francisco de As√≠s in 1776, marking the beginning of Spanish colonial presence.\n\nFollowing Mexico's independence from Spain in 1821, the area remained relatively undeveloped until the California Gold Rush in 1848. The discovery of gold transformed San Francisco from a modest settlement of about 200 residents into a bustling metropolis, with the population skyrocketing to approximately 25,000 by 1849. This rapid growth established San Francisco as a major commercial and financial center on the West Coast, officially incorporating as a city in 1850.\n\n## Impact of the 1906 Earthquake and Fire\n\nThe 1906 earthquake and the subsequent fires had a profound impact on San Francisco's infrastructure and population. Approximately 28,000 buildings were destroyed, leaving around 225,000 people homeless. The fires, exacerbated by inadequate firefighting resources, prompted a major transformation in urban planning and building codes. The rebuilding process led to significant changes in land use and demographic shifts, as many residents migrated away from the most affected areas.\n\n## Immigration and Demographic Changes\n\nImmigration has played a crucial role in shaping San Francisco's demographics. By 1860, half of the city's population was comprised of immigrants, a trend that has continued over the years. Currently, approximately 283,000 immigrants reside in San Francisco, accounting for about 35% of the city's total population. The immigrant population is diverse, with significant numbers from countries such as China, the Philippines, and Mexico, contributing to the city's rich cultural tapestry and economic development.\n\n## Economic Evolution\n\nSan Francisco's economy has evolved significantly since its founding. Initially driven by the Gold Rush, the city became a hub for various industries, including manufacturing and shipping. The 1906 earthquake prompted economic revitalization through reconstruction efforts. Over the 20th century, the economy shifted from manufacturing to services, particularly in finance and technology, with the rise of Silicon Valley solidifying San Francisco's status as a global tech hub.\n\nThe late 20th century saw the emergence of venture capital, which fueled the growth of startups and innovation. However, the city has faced challenges such as rising housing costs and income inequality, driven by the tech boom and high demand for housing.\n\n## Cultural and Artistic Movements\n\nSan Francisco has been a cradle for significant cultural and artistic movements. The Beat Generation of the 1950s, characterized by a rejection of conventional society, laid the groundwork for the counterculture movement of the 1960s. This movement, particularly in the Haight-Ashbury district, embraced peace, love, and communal living, influencing music, art, and social norms.\n\nThe city has also been pivotal in LGBTQ+ rights activism, particularly following the 1969 Stonewall Riots. San Francisco hosts one of the largest Pride parades in the world, celebrating LGBTQ+ culture and rights. Other notable movements include the Free Speech Movement at UC Berkeley and the environmental justice movement, reflecting the city's progressive values.\n\n## Historical Landmarks\n\nSan Francisco is home to numerous historical landmarks that reflect its rich history. Alcatraz Island, once a notorious federal prison, symbolizes justice and incarceration in the U.S. The Golden Gate Bridge, completed in 1937, is an iconic symbol of American engineering. Coit Tower, the Palace of Fine Arts, and Mission Dolores are other significant sites that showcase the city's artistic heritage and resilience.\n\n## Challenges and Resilience\n\nThroughout its history, San Francisco has faced numerous challenges, including natural disasters, economic fluctuations, and social issues such as homelessness and drug addiction. The 1906 earthquake was a pivotal moment that tested the city's resilience. In recent years, the housing crisis and environmental challenges have prompted ongoing efforts in urban planning and disaster preparedness.\n\n## Conclusion\n\nSan Francisco's history is a tapestry woven from diverse threads of immigration, economic transformation, cultural movements, and resilience in the face of challenges. The city's unique geography, relationship with the Pacific Ocean, and commitment to social change have shaped its identity as a vibrant urban center. As San Francisco continues to evolve, it remains a significant player in the cultural, economic, and social landscape of the United States.\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}